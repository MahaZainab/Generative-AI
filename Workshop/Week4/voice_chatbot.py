# -*- coding: utf-8 -*-
"""Voice Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AqNpeaWstrEk_2Hk6BZ3yA-GfEBYJzER
"""



"""### For voice to voice chatbot
1.  covert voice to text
2. LLM integation
3. LLM's response into voice
"""

!pip install groq

!pip install gradio groq openai-whisper pyttsx3

import os
import gradio as gr
import whisper
import pyttsx3
from groq import Groq

client = Groq(
    api_key="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",
)

!pip install gtts

import os
import gradio as gr
import whisper
from gtts import gTTS
from groq import Groq

# Set up Groq API client
client = Groq(
    api_key="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",
)

# Load Whisper model
model = whisper.load_model("base")

def chatbot(audio):
    # Transcribe the audio input using Whisper
    transcription = model.transcribe(audio)
    user_input = transcription["text"]

    # Generate a response using Llama 8B via Groq API
    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": user_input,
            }
        ],
        model="llama3-8b-8192",
    )
    response_text = chat_completion.choices[0].message.content

    # Convert the response text to speech using gTTS
    tts = gTTS(text=response_text, lang='en')
    tts.save("response.mp3")

    return response_text, "response.mp3"

# Set up Gradio interface
iface = gr.Interface(
    fn=chatbot,
    inputs=gr.Audio(type="filepath"),  # Corrected input parameters
    outputs=[gr.Textbox(), gr.Audio()],
    live=True
)

iface.launch()

!pip install gtts

